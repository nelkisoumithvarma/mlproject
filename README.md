# House Prices ‚Äì End-to-End Machine Learning Project

**Dataset:** Kaggle House Prices ‚Äì Advanced Regression Techniques

This project builds a complete regression pipeline to predict house sale prices using structured tabular data from the Kaggle House Prices dataset.

The focus is not just on model accuracy, but on proper ML workflow:
- Baseline modeling
- Feature engineering
- Pipeline-based preprocessing
- Cross-validation
- Model comparison (Bagging vs Boosting)

---

## Project Objective

Develop a robust regression model while following production-ready best practices:

- Avoid data leakage using `Pipeline` + `ColumnTransformer`
- Validate performance using 5-fold cross-validation
- Compare models fairly under identical preprocessing
- Improve performance through feature engineering and boosting

---


## üõ† Technologies & Libraries

- **Pandas** ‚Äì Feature engineering, missing value handling, DataFrame operations  
- **Scikit-learn** ‚Äì Pipeline, ColumnTransformer, RandomForestRegressor, cross-validation  
- **XGBoost** ‚Äì Gradient boosting model for performance improvement  
- **Matplotlib** ‚Äì Exploratory data visualization  
- **NumPy** ‚Äì Numerical computations  


----



## Modeling Workflow

### 1Ô∏è Baseline Model ‚Äî RandomForest (Numeric Features Only)
- Median imputation
- Simple train/validation split
- MAE: **17,024**

This establishes a clean performance benchmark.

---

### 2Ô∏è Feature Engineering + Full Pipeline ‚Äî RandomForest
Added:
- HouseAge
- RemodelAge
- TotalSF
- TotalBathrooms
- Garage & Basement indicators
- Proper categorical encoding

5-Fold Cross-Validation MAE: **17,126**

Observation:
Single split improvement was slightly optimistic ‚Äî CV provided stable evaluation.

---

### 3Ô∏è Boosting Upgrade ‚Äî XGBoost
Same preprocessing pipeline  
Tuned hyperparameters:
- 500 estimators
- Learning rate = 0.05
- max_depth = 4
- Subsample & column sampling

5-Fold Cross-Validation MAE: **14,882**

üìâ ~13% performance improvement over RandomForest.

---

## Final Results Summary

| Model |                     | CV MAE |
|-------|--------|
| RandomForest (numeric only) | 17,024 |
| RandomForest (engineered)   | 17,126 |
| XGBoost                     | **14,882** |

XGBoost selected as final model due to significantly lower cross-validated error.

---

## üß† Key Learnings

- Cross-validation prevents misleading conclusions
- Proper preprocessing pipelines are critical for reproducibility
- Feature engineering alone may not outperform strong tree ensembles
- Boosting significantly reduces bias on structured tabular data
- Controlled experimentation (changing one variable at a time) matters

---

## üóÇ Project Structure

data/ # Local dataset (not tracked in Git)
ml/ # Python training scripts
notebooks/ # Data exploration
results.md # Experiment log
README.md
.gitignore


---

## üöÄ How to Run

From project root:

```bash
python ml/baseline_model.py
python ml/train_with_features.py
python ml/train_with_xgboost.py
